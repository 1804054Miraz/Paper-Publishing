{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1804054Miraz/Paper-Publishing/blob/main/ML_apply_in_Frequency_Band_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sY0nAhQqQ4SP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import statistics as st\n",
        "import scipy.stats as scst\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "from scipy.stats import entropy\n",
        "from scipy.stats import normaltest\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import time\n",
        "from sklearn.ensemble import BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fDn37bYgSf18"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/Katz_Fd_Features_a_c.csv'\n",
        "Katz_Fd_Features_a_c = pd.read_csv(path)\n",
        "# Katz_Fd_Features_a_c = Katz_Fd_Features_a_c.replace(0, Katz_Fd_Features_a_c.mean(numeric_only=True))\n",
        "Katz_Fd_Features_a_c = Katz_Fd_Features_a_c.drop(7620)\n",
        "Katz_Fd_Features_a_c = Katz_Fd_Features_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Y9HjI-eqIbFp",
        "outputId": "f4f97ba4-36e9-424b-af46-672b366f0753"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           FP2     label\n",
              "0     1.655864  addicted\n",
              "1     1.367933  addicted\n",
              "2     1.584968  addicted\n",
              "3     1.282758  addicted\n",
              "4     1.461674  addicted\n",
              "...        ...       ...\n",
              "7616  1.819097    normal\n",
              "7617  1.538744    normal\n",
              "7618  1.716444    normal\n",
              "7619  1.556286    normal\n",
              "7620  1.598684    normal\n",
              "\n",
              "[7621 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b7d862f-3972-441a-be47-fe0ad73039ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FP2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.655864</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.367933</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.584968</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.282758</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.461674</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7616</th>\n",
              "      <td>1.819097</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7617</th>\n",
              "      <td>1.538744</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7618</th>\n",
              "      <td>1.716444</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7619</th>\n",
              "      <td>1.556286</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7620</th>\n",
              "      <td>1.598684</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7621 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b7d862f-3972-441a-be47-fe0ad73039ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b7d862f-3972-441a-be47-fe0ad73039ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b7d862f-3972-441a-be47-fe0ad73039ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4ffec25c-c7c4-4762-927a-1ec45abff8e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ffec25c-c7c4-4762-927a-1ec45abff8e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4ffec25c-c7c4-4762-927a-1ec45abff8e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Katz_Fd_Features_a_c",
              "summary": "{\n  \"name\": \"Katz_Fd_Features_a_c\",\n  \"rows\": 7621,\n  \"fields\": [\n    {\n      \"column\": \"FP2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27329629383393494,\n        \"min\": 0.0,\n        \"max\": 3.4187971117197624,\n        \"num_unique_values\": 7585,\n        \"samples\": [\n          1.250958729770458,\n          1.6234319784347897,\n          1.5688035399593483\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"normal\",\n          \"addicted\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "Katz_Fd_Features_a_c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = Katz_Fd_Features_a_c.iloc[:, :-1]\n",
        "y = Katz_Fd_Features_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IEuM2jBSVWK",
        "outputId": "8a5ade4f-0555-438f-c264-c82fd8367214"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.578401784542711\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.5651489305865371\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.526702532476053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.5441543104579452\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 4ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.5643632848646213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = Katz_Fd_Features_a_c.iloc[:, :-1]\n",
        "y = Katz_Fd_Features_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnsqVy3aSkyL",
        "outputId": "1af15b63-fe61-43af-fac3-d24ea764f47f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.5623933866946595\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.5724970476315444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = Katz_Fd_Features_a_c.iloc[:, :-1]\n",
        "y = Katz_Fd_Features_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-v9MlIvSh_v",
        "outputId": "67a8fec4-2bb8-49d0-e0a2-33a3918eb78e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 255\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.5706614654819523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5eoaZZ78SGpE"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/merged_alpha_a_c.csv'\n",
        "merged_alpha_a_c = pd.read_csv(path)\n",
        "# merged_alpha_a_c=merged_alpha_a_c.drop(columns=['X_PS','Y_PS','nd_PS'], axis=False)\n",
        "# merged_alpha_a_c = merged_alpha_a_c.replace(0, merged_alpha_a_c.mean(numeric_only=True))\n",
        "merged_alpha_a_c = merged_alpha_a_c.drop(7620)\n",
        "merged_alpha_a_c = merged_alpha_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3mvtRVBySN0E"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/merged_beta_a_c.csv'\n",
        "merged_beta_a_c = pd.read_csv(path)\n",
        "# merged_beta_a_c=merged_beta_a_c.drop(columns=['X_PS','Y_PS','nd_PS'], axis=False)\n",
        "# merged_beta_a_c = merged_beta_a_c.replace(0, merged_beta_a_c.mean(numeric_only=True))\n",
        "merged_beta_a_c = merged_beta_a_c.drop(7620)\n",
        "merged_beta_a_c = merged_beta_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZkjfWVbbTcFq"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/merged_delta_a_c.csv'\n",
        "merged_delta_a_c = pd.read_csv(path)\n",
        "# merged_delta_a_c=merged_delta_a_c.drop(columns=['X_PS','Y_PS','nd_PS'], axis=False)\n",
        "# merged_delta_a_c = merged_delta_a_c.replace(0, merged_delta_a_c.mean(numeric_only=True))\n",
        "merged_delta_a_c = merged_delta_a_c.drop(7620)\n",
        "merged_delta_a_c = merged_delta_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GdP5jgLva3cl"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/merged_gamma_a_c.csv'\n",
        "merged_gamma_a_c = pd.read_csv(path)\n",
        "# merged_gamma_a_c=merged_gamma_a_c.drop(columns=['X_PS','Y_PS','nd_PS'], axis=False)\n",
        "# merged_gamma_a_c = merged_gamma_a_c.replace(0, merged_gamma_a_c.mean(numeric_only=True))\n",
        "merged_gamma_a_c = merged_gamma_a_c.drop(7620)\n",
        "merged_gamma_a_c = merged_gamma_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y18FOngfTfqp"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Dataset/Features/merged_theta_a_c.csv'\n",
        "merged_theta_a_c = pd.read_csv(path)\n",
        "# merged_theta_a_c=merged_theta_a_c.drop(columns=['X_PS','Y_PS','nd_PS'], axis=False)\n",
        "# merged_theta_a_c = merged_theta_a_c.replace(0, merged_theta_a_c.mean(numeric_only=True))\n",
        "merged_theta_a_c = merged_theta_a_c.drop(7620)\n",
        "merged_theta_a_c = merged_theta_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "whq8PWNFs-rX",
        "outputId": "13728775-37be-4507-811c-94f56563f803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        FP1_PS     FP2_PS      F7_PS       F8_PS    AF1_PS     AF2_PS  \\\n",
              "0     5.867765  16.136485  14.419578  107.004679  4.771000  10.412779   \n",
              "1     4.788753  11.568614  14.381443   75.048537  2.176075   8.500067   \n",
              "2     6.297802  29.544646  13.712807   97.931325  4.131878  13.930406   \n",
              "3     4.348968   9.195677  10.247385   62.184384  2.506088   6.808748   \n",
              "4     7.698942  15.086431  20.116347   61.128746  6.571280   8.982171   \n",
              "...        ...        ...        ...         ...       ...        ...   \n",
              "7616  1.684572   1.623761   9.112097    2.782111  1.144432   0.884217   \n",
              "7617  2.437103   2.691105   9.169849    2.129555  1.467451   1.610917   \n",
              "7618  0.897495   1.245264   3.604244    3.566347  1.042233   1.365339   \n",
              "7619  1.251944   1.448347   6.375324    4.212090  0.632365   0.651843   \n",
              "7620  3.617902   3.738929   5.852530    5.559126  3.810300   3.586122   \n",
              "\n",
              "         FZ_PS     F4_PS     F3_PS    FC6_PS  ...     PO8_PS    FCZ_PS  \\\n",
              "0     2.264702  4.967259  4.718617  2.227520  ...  31.407003  0.639929   \n",
              "1     1.558222  4.906503  3.750594  1.237990  ...  31.837468  0.481474   \n",
              "2     1.443750  4.954165  4.627834  2.631432  ...  39.507001  0.403271   \n",
              "3     1.321086  3.921911  3.681952  1.816489  ...  30.478540  0.450695   \n",
              "4     3.578223  4.040258  6.796313  2.204725  ...  55.854653  1.011451   \n",
              "...        ...       ...       ...       ...  ...        ...       ...   \n",
              "7616  1.269359  0.880600  1.205278  0.957745  ...   2.408939  0.174153   \n",
              "7617  0.581611  2.188392  2.580653  1.218395  ...   5.024580  0.227139   \n",
              "7618  0.834950  1.492575  0.902565  1.093884  ...   3.040292  0.398802   \n",
              "7619  0.297628  0.621490  0.765336  1.009508  ...   2.070562  0.132006   \n",
              "7620  3.760634  3.979930  5.524031  3.403491  ...   4.453148  3.155204   \n",
              "\n",
              "        POZ_PS     OZ_PS     P2_PS     P1_PS    CPZ_PS     nd_PS      Y_PS  \\\n",
              "0     3.175360  7.257440  1.171091  1.727645  0.303052  5.626485  6.090333   \n",
              "1     1.360249  3.002968  0.452312  0.873387  0.169722  4.693551  3.472028   \n",
              "2     1.391440  2.908869  0.790318  1.077771  0.227142  6.266331  3.612442   \n",
              "3     1.121842  2.162650  0.753562  0.999099  0.180620  4.211706  6.950914   \n",
              "4     3.663678  5.646425  1.918202  2.340282  0.437771  7.413797  7.107321   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "7616  0.867184  1.234990  0.535141  0.633024  0.261792  1.058754  6.116856   \n",
              "7617  3.721454  4.377724  1.713244  1.729774  0.453551  5.866098  3.061931   \n",
              "7618  3.180998  2.880482  2.019003  1.853624  0.785453  2.408728  3.512361   \n",
              "7619  1.298873  1.126842  0.855509  0.550932  0.247462  1.363607  3.663259   \n",
              "7620  4.027627  3.094130  2.927279  3.796067  3.164739  3.551407  9.684160   \n",
              "\n",
              "         label  \n",
              "0     addicted  \n",
              "1     addicted  \n",
              "2     addicted  \n",
              "3     addicted  \n",
              "4     addicted  \n",
              "...        ...  \n",
              "7616    normal  \n",
              "7617    normal  \n",
              "7618    normal  \n",
              "7619    normal  \n",
              "7620    normal  \n",
              "\n",
              "[7621 rows x 65 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3fd3a2a-2781-4525-a318-2046de135945\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FP1_PS</th>\n",
              "      <th>FP2_PS</th>\n",
              "      <th>F7_PS</th>\n",
              "      <th>F8_PS</th>\n",
              "      <th>AF1_PS</th>\n",
              "      <th>AF2_PS</th>\n",
              "      <th>FZ_PS</th>\n",
              "      <th>F4_PS</th>\n",
              "      <th>F3_PS</th>\n",
              "      <th>FC6_PS</th>\n",
              "      <th>...</th>\n",
              "      <th>PO8_PS</th>\n",
              "      <th>FCZ_PS</th>\n",
              "      <th>POZ_PS</th>\n",
              "      <th>OZ_PS</th>\n",
              "      <th>P2_PS</th>\n",
              "      <th>P1_PS</th>\n",
              "      <th>CPZ_PS</th>\n",
              "      <th>nd_PS</th>\n",
              "      <th>Y_PS</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.867765</td>\n",
              "      <td>16.136485</td>\n",
              "      <td>14.419578</td>\n",
              "      <td>107.004679</td>\n",
              "      <td>4.771000</td>\n",
              "      <td>10.412779</td>\n",
              "      <td>2.264702</td>\n",
              "      <td>4.967259</td>\n",
              "      <td>4.718617</td>\n",
              "      <td>2.227520</td>\n",
              "      <td>...</td>\n",
              "      <td>31.407003</td>\n",
              "      <td>0.639929</td>\n",
              "      <td>3.175360</td>\n",
              "      <td>7.257440</td>\n",
              "      <td>1.171091</td>\n",
              "      <td>1.727645</td>\n",
              "      <td>0.303052</td>\n",
              "      <td>5.626485</td>\n",
              "      <td>6.090333</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.788753</td>\n",
              "      <td>11.568614</td>\n",
              "      <td>14.381443</td>\n",
              "      <td>75.048537</td>\n",
              "      <td>2.176075</td>\n",
              "      <td>8.500067</td>\n",
              "      <td>1.558222</td>\n",
              "      <td>4.906503</td>\n",
              "      <td>3.750594</td>\n",
              "      <td>1.237990</td>\n",
              "      <td>...</td>\n",
              "      <td>31.837468</td>\n",
              "      <td>0.481474</td>\n",
              "      <td>1.360249</td>\n",
              "      <td>3.002968</td>\n",
              "      <td>0.452312</td>\n",
              "      <td>0.873387</td>\n",
              "      <td>0.169722</td>\n",
              "      <td>4.693551</td>\n",
              "      <td>3.472028</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.297802</td>\n",
              "      <td>29.544646</td>\n",
              "      <td>13.712807</td>\n",
              "      <td>97.931325</td>\n",
              "      <td>4.131878</td>\n",
              "      <td>13.930406</td>\n",
              "      <td>1.443750</td>\n",
              "      <td>4.954165</td>\n",
              "      <td>4.627834</td>\n",
              "      <td>2.631432</td>\n",
              "      <td>...</td>\n",
              "      <td>39.507001</td>\n",
              "      <td>0.403271</td>\n",
              "      <td>1.391440</td>\n",
              "      <td>2.908869</td>\n",
              "      <td>0.790318</td>\n",
              "      <td>1.077771</td>\n",
              "      <td>0.227142</td>\n",
              "      <td>6.266331</td>\n",
              "      <td>3.612442</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.348968</td>\n",
              "      <td>9.195677</td>\n",
              "      <td>10.247385</td>\n",
              "      <td>62.184384</td>\n",
              "      <td>2.506088</td>\n",
              "      <td>6.808748</td>\n",
              "      <td>1.321086</td>\n",
              "      <td>3.921911</td>\n",
              "      <td>3.681952</td>\n",
              "      <td>1.816489</td>\n",
              "      <td>...</td>\n",
              "      <td>30.478540</td>\n",
              "      <td>0.450695</td>\n",
              "      <td>1.121842</td>\n",
              "      <td>2.162650</td>\n",
              "      <td>0.753562</td>\n",
              "      <td>0.999099</td>\n",
              "      <td>0.180620</td>\n",
              "      <td>4.211706</td>\n",
              "      <td>6.950914</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.698942</td>\n",
              "      <td>15.086431</td>\n",
              "      <td>20.116347</td>\n",
              "      <td>61.128746</td>\n",
              "      <td>6.571280</td>\n",
              "      <td>8.982171</td>\n",
              "      <td>3.578223</td>\n",
              "      <td>4.040258</td>\n",
              "      <td>6.796313</td>\n",
              "      <td>2.204725</td>\n",
              "      <td>...</td>\n",
              "      <td>55.854653</td>\n",
              "      <td>1.011451</td>\n",
              "      <td>3.663678</td>\n",
              "      <td>5.646425</td>\n",
              "      <td>1.918202</td>\n",
              "      <td>2.340282</td>\n",
              "      <td>0.437771</td>\n",
              "      <td>7.413797</td>\n",
              "      <td>7.107321</td>\n",
              "      <td>addicted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7616</th>\n",
              "      <td>1.684572</td>\n",
              "      <td>1.623761</td>\n",
              "      <td>9.112097</td>\n",
              "      <td>2.782111</td>\n",
              "      <td>1.144432</td>\n",
              "      <td>0.884217</td>\n",
              "      <td>1.269359</td>\n",
              "      <td>0.880600</td>\n",
              "      <td>1.205278</td>\n",
              "      <td>0.957745</td>\n",
              "      <td>...</td>\n",
              "      <td>2.408939</td>\n",
              "      <td>0.174153</td>\n",
              "      <td>0.867184</td>\n",
              "      <td>1.234990</td>\n",
              "      <td>0.535141</td>\n",
              "      <td>0.633024</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>1.058754</td>\n",
              "      <td>6.116856</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7617</th>\n",
              "      <td>2.437103</td>\n",
              "      <td>2.691105</td>\n",
              "      <td>9.169849</td>\n",
              "      <td>2.129555</td>\n",
              "      <td>1.467451</td>\n",
              "      <td>1.610917</td>\n",
              "      <td>0.581611</td>\n",
              "      <td>2.188392</td>\n",
              "      <td>2.580653</td>\n",
              "      <td>1.218395</td>\n",
              "      <td>...</td>\n",
              "      <td>5.024580</td>\n",
              "      <td>0.227139</td>\n",
              "      <td>3.721454</td>\n",
              "      <td>4.377724</td>\n",
              "      <td>1.713244</td>\n",
              "      <td>1.729774</td>\n",
              "      <td>0.453551</td>\n",
              "      <td>5.866098</td>\n",
              "      <td>3.061931</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7618</th>\n",
              "      <td>0.897495</td>\n",
              "      <td>1.245264</td>\n",
              "      <td>3.604244</td>\n",
              "      <td>3.566347</td>\n",
              "      <td>1.042233</td>\n",
              "      <td>1.365339</td>\n",
              "      <td>0.834950</td>\n",
              "      <td>1.492575</td>\n",
              "      <td>0.902565</td>\n",
              "      <td>1.093884</td>\n",
              "      <td>...</td>\n",
              "      <td>3.040292</td>\n",
              "      <td>0.398802</td>\n",
              "      <td>3.180998</td>\n",
              "      <td>2.880482</td>\n",
              "      <td>2.019003</td>\n",
              "      <td>1.853624</td>\n",
              "      <td>0.785453</td>\n",
              "      <td>2.408728</td>\n",
              "      <td>3.512361</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7619</th>\n",
              "      <td>1.251944</td>\n",
              "      <td>1.448347</td>\n",
              "      <td>6.375324</td>\n",
              "      <td>4.212090</td>\n",
              "      <td>0.632365</td>\n",
              "      <td>0.651843</td>\n",
              "      <td>0.297628</td>\n",
              "      <td>0.621490</td>\n",
              "      <td>0.765336</td>\n",
              "      <td>1.009508</td>\n",
              "      <td>...</td>\n",
              "      <td>2.070562</td>\n",
              "      <td>0.132006</td>\n",
              "      <td>1.298873</td>\n",
              "      <td>1.126842</td>\n",
              "      <td>0.855509</td>\n",
              "      <td>0.550932</td>\n",
              "      <td>0.247462</td>\n",
              "      <td>1.363607</td>\n",
              "      <td>3.663259</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7620</th>\n",
              "      <td>3.617902</td>\n",
              "      <td>3.738929</td>\n",
              "      <td>5.852530</td>\n",
              "      <td>5.559126</td>\n",
              "      <td>3.810300</td>\n",
              "      <td>3.586122</td>\n",
              "      <td>3.760634</td>\n",
              "      <td>3.979930</td>\n",
              "      <td>5.524031</td>\n",
              "      <td>3.403491</td>\n",
              "      <td>...</td>\n",
              "      <td>4.453148</td>\n",
              "      <td>3.155204</td>\n",
              "      <td>4.027627</td>\n",
              "      <td>3.094130</td>\n",
              "      <td>2.927279</td>\n",
              "      <td>3.796067</td>\n",
              "      <td>3.164739</td>\n",
              "      <td>3.551407</td>\n",
              "      <td>9.684160</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7621 rows Ã— 65 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3fd3a2a-2781-4525-a318-2046de135945')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3fd3a2a-2781-4525-a318-2046de135945 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3fd3a2a-2781-4525-a318-2046de135945');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e99cf35a-6276-49b8-8e0a-bf83f14a4f5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e99cf35a-6276-49b8-8e0a-bf83f14a4f5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e99cf35a-6276-49b8-8e0a-bf83f14a4f5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "merged_gamma_a_c"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "merged_gamma_a_c"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hoGmHFBH0gSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku7Cl5MH3P_V"
      },
      "outputs": [],
      "source": [
        "# merged_gamma_katz_a_c = merged_gamma_katz_a_c.drop(7620)\n",
        "# If you want to reset the index after removing the row\n",
        "# merged_gamma_katz_a_c = merged_gamma_katz_a_c.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGLVa6msbWzm"
      },
      "outputs": [],
      "source": [
        "# merged_beta_gamma_a_c = pd.concat([merged_beta_a_c,merged_gamma_a_c], axis=1)\n",
        "# # merged_beta_gamma_a_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EerNUgDSbzXj"
      },
      "outputs": [],
      "source": [
        "# merged_beta_gamma_a_c.drop('label', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyli-i4Fb4hs"
      },
      "outputs": [],
      "source": [
        "# merged_beta_gamma_a_c['label']='normal'\n",
        "# merged_beta_gamma_a_c.loc[0:3810, 'label'] = 'addicted'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIIRo0vvdbWh",
        "outputId": "826a57cd-c0af-4187-ff24-4ffa16720ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.7289069675895552\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.7857236583125574\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.7537068626164546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.7299566985959848\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.6499103896416618\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_alpha_a_c.iloc[:, :-1]\n",
        "y = merged_alpha_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_alpha_a_c.iloc[:, :-1]\n",
        "y = merged_alpha_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS1Wqec10r7F",
        "outputId": "d0e15052-80c5-4ff8-849b-cec659cc32ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008421 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005683 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005731 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005839 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005676 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005621 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009177 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008914 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010214 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.7770633775095132\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.7876919039496129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_alpha_a_c.iloc[:, :-1]\n",
        "y = merged_alpha_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpneJwR006A0",
        "outputId": "e900ac79-017b-462b-8c41-7432224a7d3f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005850 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005879 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005948 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005837 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005904 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007321 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005875 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005971 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006168 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005892 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.7899223606223534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVXo75KQPXNe",
        "outputId": "df2320b0-a8f0-470b-c57b-89124d76497a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.8375541267550191\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.9098543498228578\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.8632725364125443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.8913528408345361\n",
            "24/24 [==============================] - 0s 5ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 5ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.8021241610853689\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_beta_a_c.iloc[:, :-1]\n",
        "y = merged_beta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_beta_a_c.iloc[:, :-1]\n",
        "y = merged_beta_a_c.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqPoQBzSLFTw",
        "outputId": "092c586c-e01b-4e2c-ba1c-8b0322f8eafc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005583 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005759 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005785 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005852 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005660 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005792 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008841 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008871 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013218 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005806 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.8996194725101693\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.9028998819052618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_beta_a_c.iloc[:, :-1]\n",
        "y = merged_beta_a_c.iloc[:, -1]\n",
        "\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaYicnsgLGhm",
        "outputId": "85a13b00-c057-47cb-a730-4dd859440d53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007779 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009227 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006088 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005938 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009374 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005920 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006017 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008098 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.9119530930193358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkoXJjUyPbLH",
        "outputId": "f5b46274-ec2a-4a4c-a244-c39a3fdf630d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.6875738092113896\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.7479333420810917\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.7144731662511481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.6538512006298386\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 4ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.5365470944572295\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_delta_a_c.iloc[:, :-1]\n",
        "y = merged_delta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_delta_a_c.iloc[:, :-1]\n",
        "y = merged_delta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mALqu0dMebs",
        "outputId": "8a57cef3-e4ea-4f2d-f9ba-2ee0077ff4a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009491 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010543 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005817 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005554 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005758 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005763 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005756 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005906 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005753 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005872 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.7442592835585881\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.7576433538905656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_delta_a_c.iloc[:, :-1]\n",
        "y = merged_delta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjeCNGW9MiNM",
        "outputId": "266d6519-7ff1-4d9c-a688-730cfd647bf5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005805 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014981 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005785 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006135 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008960 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005919 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006009 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005859 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009235 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006136 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.7615803758475144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YXs4pA_PgYp",
        "outputId": "785cbefd-6121-42be-9790-8da7e83d7bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.8463456239338669\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.9574858942396011\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.9278309933079648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.9430520929011941\n",
            "24/24 [==============================] - 1s 7ms/step\n",
            "24/24 [==============================] - 1s 9ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 1s 8ms/step\n",
            "24/24 [==============================] - 1s 6ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.9190390192051682\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_gamma_a_c.iloc[:, :-1]\n",
        "y = merged_gamma_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost lightgbm catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CJEKDFJvxl7",
        "outputId": "b0d6fb9f-5658-4b7d-eb08-7fe7457b64e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_gamma_a_c.iloc[:, :-1]\n",
        "y = merged_gamma_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSg8pRAYvtXr",
        "outputId": "e64da69b-4108-42e3-94bc-23776d112443"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010187 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005845 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008023 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005894 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010186 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009726 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010048 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008461 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024707 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009642 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.9523684555832568\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.958404408870227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_gamma_a_c.iloc[:, :-1]\n",
        "y = merged_gamma_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZIdc5DxwUuN",
        "outputId": "87a77693-b996-4bcb-db9b-1d28c52e2c68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009413 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005811 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005941 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010640 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005849 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009438 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005862 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009224 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005901 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.9595860723831539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KCJMPkfHPmuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ab39f8-deba-4265-9426-0fdb916d3f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Accuracy: 0.7380921138958142\n",
            "\n",
            "XGBoost Metrics:\n",
            "Accuracy: 0.7803437869046057\n",
            "\n",
            "Random Forest Metrics:\n",
            "Accuracy: 0.7500328040939509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ensemble Subspace of k-Nearest Neighbors Metrics:\n",
            "Accuracy: 0.7228710143025849\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 3ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n",
            "\n",
            "LSTM Metrics:\n",
            "LSTM Model Accuracy: 0.5785413291228504\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_theta_a_c.iloc[:, :-1]\n",
        "y = merged_theta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "# start_time = time.time()\n",
        "svm_classifier = make_pipeline(StandardScaler(), SVC())\n",
        "svm_predictions = cross_val_predict(svm_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "svm_accuracy = accuracy_score(y_encoded, svm_predictions)\n",
        "# svm_precision = precision_score(y_encoded, svm_predictions)\n",
        "# svm_recall = recall_score(y_encoded, svm_predictions)\n",
        "# svm_f1 = f1_score(y_encoded, svm_predictions)\n",
        "# svm_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Accuracy:\", svm_accuracy)\n",
        "# print(\"Precision:\", svm_precision)\n",
        "# print(\"Recall:\", svm_recall)\n",
        "# print(\"F1 Score:\", svm_f1)\n",
        "# print(\"Processing Time:\", svm_processing_time)\n",
        "\n",
        "# XGBoost\n",
        "# start_time = time.time()\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_predictions = cross_val_predict(xgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "xgb_accuracy = accuracy_score(y_encoded, xgb_predictions)\n",
        "# xgb_precision = precision_score(y_encoded, xgb_predictions)\n",
        "# xgb_recall = recall_score(y_encoded, xgb_predictions)\n",
        "# xgb_f1 = f1_score(y_encoded, xgb_predictions)\n",
        "# xgb_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nXGBoost Metrics:\")\n",
        "print(\"Accuracy:\", xgb_accuracy)\n",
        "# print(\"Precision:\", xgb_precision)\n",
        "# print(\"Recall:\", xgb_recall)\n",
        "# print(\"F1 Score:\", xgb_f1)\n",
        "# print(\"Processing Time:\", xgb_processing_time)\n",
        "\n",
        "# Random Forest\n",
        "# start_time = time.time()\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_predictions = cross_val_predict(rf_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "rf_accuracy = accuracy_score(y_encoded, rf_predictions)\n",
        "# rf_precision = precision_score(y_encoded, rf_predictions)\n",
        "# rf_recall = recall_score(y_encoded, rf_predictions)\n",
        "# rf_f1 = f1_score(y_encoded, rf_predictions)\n",
        "# rf_processing_time = time.time() - start_time\n",
        "\n",
        "print(\"\\nRandom Forest Metrics:\")\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "# print(\"Precision:\", rf_precision)\n",
        "# print(\"Recall:\", rf_recall)\n",
        "# print(\"F1 Score:\", rf_f1)\n",
        "# print(\"Processing Time:\", rf_processing_time)\n",
        "\n",
        "\n",
        "# Ensemble Subspace of k-Nearest Neighbors\n",
        "# start_time_ensemble_knn = time.time()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "ensemble_knn_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "ensemble_knn_scores = cross_val_predict(ensemble_knn_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "ensemble_knn_accuracy = accuracy_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_precision = precision_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_recall = recall_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_f1 = f1_score(y_encoded, ensemble_knn_scores)\n",
        "# ensemble_knn_processing_time = time.time() - start_time_ensemble_knn\n",
        "\n",
        "print(\"\\nEnsemble Subspace of k-Nearest Neighbors Metrics:\")\n",
        "print(\"Accuracy:\", ensemble_knn_accuracy)\n",
        "# print(\"Precision:\", ensemble_knn_precision)\n",
        "# print(\"Recall:\", ensemble_knn_recall)\n",
        "# print(\"F1 Score:\", ensemble_knn_f1)\n",
        "# print(\"Processing Time:\", ensemble_knn_processing_time)\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Convert DataFrame to numpy array\n",
        "X_np = X.values\n",
        "X_np = X_np.reshape((X_np.shape[0], 1, X_np.shape[1]))\n",
        "\n",
        "# 10-fold cross-validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_scores = []\n",
        "# precision_scores = []\n",
        "# recall_scores = []\n",
        "# f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(X_np):\n",
        "    X_train, X_test = X_np[train_idx], X_np[test_idx]\n",
        "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
        "\n",
        "    # Create and fit LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = create_lstm_model(input_shape)\n",
        "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = lstm_model.predict(X_test)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Evaluate metrics\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    # precision_scores.append(precision_score(y_test, y_pred))\n",
        "    # recall_scores.append(recall_score(y_test, y_pred))\n",
        "    # f1_scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Metrics:\")\n",
        "print(\"LSTM Model Accuracy:\", np.mean(accuracy_scores))\n",
        "# print(\"LSTM Model Precision:\", np.mean(precision_scores))\n",
        "# print(\"LSTM Model Recall:\", np.mean(recall_scores))\n",
        "# print(\"LSTM Model F1 Score:\", np.mean(f1_scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_theta_a_c.iloc[:, :-1]\n",
        "y = merged_theta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# LightGBM\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "lgb_predictions = cross_val_predict(lgb_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "lgb_accuracy = accuracy_score(y_encoded, lgb_predictions)\n",
        "# lgb_precision = precision_score(y_encoded, lgb_predictions)\n",
        "# lgb_recall = recall_score(y_encoded, lgb_predictions)\n",
        "# lgb_f1 = f1_score(y_encoded, lgb_predictions)\n",
        "\n",
        "print(\"\\nLightGBM Metrics:\")\n",
        "print(\"Accuracy:\", lgb_accuracy)\n",
        "# print(\"Precision:\", lgb_precision)\n",
        "# print(\"Recall:\", lgb_recall)\n",
        "# print(\"F1 Score:\", lgb_f1)\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# CatBoost\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "catboost_predictions = cross_val_predict(catboost_classifier, X, y_encoded, cv=KFold(n_splits=10, shuffle=True, random_state=42))\n",
        "catboost_accuracy = accuracy_score(y_encoded, catboost_predictions)\n",
        "# catboost_precision = precision_score(y_encoded, catboost_predictions)\n",
        "# catboost_recall = recall_score(y_encoded, catboost_predictions)\n",
        "# catboost_f1 = f1_score(y_encoded, catboost_predictions)\n",
        "\n",
        "print(\"\\nCatBoost Metrics:\")\n",
        "print(\"Accuracy:\", catboost_accuracy)\n",
        "# print(\"Precision:\", catboost_precision)\n",
        "# print(\"Recall:\", catboost_recall)\n",
        "# print(\"F1 Score:\", catboost_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Cdiu-QyGlm",
        "outputId": "b7c5b4f3-d450-4bc8-ae06-bee8497e873d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011604 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009102 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010257 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008924 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006794 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005666 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005695 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005648 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005673 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "LightGBM Metrics:\n",
            "Accuracy: 0.7819183834142501\n",
            "\n",
            "CatBoost Metrics:\n",
            "Accuracy: 0.7874294711980055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'new_dataframe' is your DataFrame\n",
        "X = merged_theta_a_c.iloc[:, :-1]\n",
        "y = merged_theta_a_c.iloc[:, -1]\n",
        "\n",
        "# Encode the target variable if it's categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Define individual models\n",
        "xgb_classifier = XGBClassifier()\n",
        "lgb_classifier = lgb.LGBMClassifier()\n",
        "catboost_classifier = CatBoostClassifier(verbose=0)\n",
        "\n",
        "# Define voting classifier\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('lgb', lgb_classifier),\n",
        "        ('catboost', catboost_classifier)\n",
        "    ],\n",
        "    voting='soft'  # 'hard' for majority voting, 'soft' for weighted probabilities\n",
        ")\n",
        "\n",
        "# Define k-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "scoring = {'accuracy': make_scorer(accuracy_score)}\n",
        "          #  'precision': make_scorer(precision_score),\n",
        "          #  'recall': make_scorer(recall_score),\n",
        "          #  'f1': make_scorer(f1_score)\n",
        "\n",
        "\n",
        "voting_scores = cross_validate(voting_classifier, X, y_encoded, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nVoting Ensemble Metrics:\")\n",
        "print(\"Accuracy:\", np.mean(voting_scores['test_accuracy']))\n",
        "# print(\"Precision:\", np.mean(voting_scores['test_precision']))\n",
        "# print(\"Recall:\", np.mean(voting_scores['test_recall']))\n",
        "# print(\"F1 Score:\", np.mean(voting_scores['test_f1']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPv-IQUEyOQX",
        "outputId": "56745338-c6ee-4530-d5a7-99aefb185084"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3438, number of negative: 3420\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005628 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6858, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501312 -> initscore=0.005249\n",
            "[LightGBM] [Info] Start training from score 0.005249\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009545 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005737 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3420, number of negative: 3439\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005793 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498615 -> initscore=-0.005540\n",
            "[LightGBM] [Info] Start training from score -0.005540\n",
            "[LightGBM] [Info] Number of positive: 3429, number of negative: 3430\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005874 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499927 -> initscore=-0.000292\n",
            "[LightGBM] [Info] Start training from score -0.000292\n",
            "[LightGBM] [Info] Number of positive: 3416, number of negative: 3443\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005950 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498032 -> initscore=-0.007873\n",
            "[LightGBM] [Info] Start training from score -0.007873\n",
            "[LightGBM] [Info] Number of positive: 3421, number of negative: 3438\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005772 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498761 -> initscore=-0.004957\n",
            "[LightGBM] [Info] Start training from score -0.004957\n",
            "[LightGBM] [Info] Number of positive: 3445, number of negative: 3414\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005865 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502260 -> initscore=0.009039\n",
            "[LightGBM] [Info] Start training from score 0.009039\n",
            "[LightGBM] [Info] Number of positive: 3430, number of negative: 3429\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005794 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500073 -> initscore=0.000292\n",
            "[LightGBM] [Info] Start training from score 0.000292\n",
            "[LightGBM] [Info] Number of positive: 3426, number of negative: 3433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005739 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 16320\n",
            "[LightGBM] [Info] Number of data points in the train set: 6859, number of used features: 64\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499490 -> initscore=-0.002041\n",
            "[LightGBM] [Info] Start training from score -0.002041\n",
            "\n",
            "Voting Ensemble Metrics:\n",
            "Accuracy: 0.7914942398255264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eudBasxFYOO6"
      },
      "outputs": [],
      "source": [
        "merged_alpha_Katz_a_c = pd.concat([merged_alpha_a_c,Katz_Fd_Features_a_c], axis=1)\n",
        "merged_alpha_Katz_a_c.drop('label', axis=1, inplace=True)\n",
        "merged_alpha_Katz_a_c['label']='normal'\n",
        "merged_alpha_Katz_a_c.loc[0:3810, 'label'] = 'addicted'\n",
        "merged_alpha_Katz_a_c\n",
        "\n",
        "merged_beta_Katz_a_c = pd.concat([merged_beta_a_c,Katz_Fd_Features_a_c], axis=1)\n",
        "merged_beta_Katz_a_c.drop('label', axis=1, inplace=True)\n",
        "merged_beta_Katz_a_c['label']='normal'\n",
        "merged_beta_Katz_a_c.loc[0:3810, 'label'] = 'addicted'\n",
        "# merged_beta_Katz_a_c\n",
        "\n",
        "merged_gamma_Katz_a_c = pd.concat([merged_gamma_a_c,Katz_Fd_Features_a_c], axis=1)\n",
        "merged_gamma_Katz_a_c.drop('label', axis=1, inplace=True)\n",
        "merged_gamma_Katz_a_c['label']='normal'\n",
        "merged_gamma_Katz_a_c.loc[0:3810, 'label'] = 'addicted'\n",
        "\n",
        "merged_delta_Katz_a_c = pd.concat([merged_delta_a_c,Katz_Fd_Features_a_c], axis=1)\n",
        "merged_delta_Katz_a_c.drop('label', axis=1, inplace=True)\n",
        "merged_delta_Katz_a_c['label']='normal'\n",
        "merged_delta_Katz_a_c.loc[0:3810, 'label'] = 'addicted'\n",
        "\n",
        "merged_theta_Katz_a_c = pd.concat([merged_theta_a_c,Katz_Fd_Features_a_c], axis=1)\n",
        "merged_theta_Katz_a_c.drop('label', axis=1, inplace=True)\n",
        "merged_theta_Katz_a_c['label']='normal'\n",
        "merged_theta_Katz_a_c.loc[0:3810, 'label'] = 'addicted'\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MXOWJizGXN3hmjae9hrvJwIfgl9CYouF",
      "authorship_tag": "ABX9TyO9KfTG26VGWl1pNijX9oka",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}